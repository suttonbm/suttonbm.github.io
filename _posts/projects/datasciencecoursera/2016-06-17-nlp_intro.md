---
title: "Natural Language Processing"
author: "suttonbm"
date: 2016-06-20
author: suttonbm
layout: post
categories:
  - projects
tags:
  - coursera
  - data.science
  - R
project: datasciencecoursera.capstone
excerpt: >
  Reading notes for Natural Language Processing NLP
---



## Introduction
Today's goal is to take a look at the text corpus to be used in the capstone project, as well as to gain a working familiarity with natural language processing, or NLP.  Let's jump right in.

The baseline data to be used in the capstone is a corpus called [HC Corpora](http://www.corpora.heliohost.org/).  Coursera/JHU have kindly made available a zip file containing corpora from English, German, Russian, and Finnish.  Each corpus contains data from blogs, news, and twitter.  Data is provided in a raw form - just text.  Any analysis will require some sort of preprocessing before anything useful can be done with the data.

Small aside - I could see it being fun to repeat this whole project with a corpus of language derived from a show or something.  Would be fun to have a Captain Picard predictive keyboard...

## Alternative/Additional Corpora
There are quite a few additional corpora openly available on the web.  I'm not sure whether it will make sense to include these data in the analysis for this course.

#### Google Books
Google has some outrageously large quantity of books processed down into n-grams and available on the web for anyone to use.  I'm not sure how big the total corpus comes to, but I probably won't be making use of this just due to sheer size.
[Google N-Grams Corpus](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html)

#### American National Corpus
Another corpus that is a bit more reasonable in size is the American National Corpus, weighing in at 15 million words.  There's also a manually annotated corpus of 500,000 words available for use.
[Open ANC](http://www.anc.org/data/oanc/download/)
[MASC](http://www.anc.org/data/masc/downloads/)

#### Brigham Young Corpus
Brigham Young University also has a fairly large corpus freely available on the web:
[BYU Corpus](http://corpus.byu.edu/full-text/)

## Text Mining Preprocessing
Preparing for text mining or analysis of a selected corpus is roughly a three-step process:

  1. Obtain the data
  2. Preprocess the data
  3. Transform the data to a structured format

Preprocessing the data aims to prepare the raw data for transformation.  Some common preprocessing steps may include:

  * Whitespace removal
  * Stopword removal
  * Word stemming
  * Punctuation removal

Stopwords are defined as words which (generally) appear extremely frequently in the corpus and may provide little predictive information.  In other words, they appear in combination with many words, making it very difficult to do prediction with high accuracy.

Word stemming is essentially clustering words with identical meaning but different suffixes, prefixes, etc.  For example, "ducks" and "duck" would be categorized as the same word.

## Getting Started in R
There are multiple frameworks in R providing language processing functionality.  However, for now I'm going to explore the `tm` package.

> We present a text mining framework ... cetered around the new extension package `tm`.  This open source package ... provides the basic infrastructure necessary to organize, transform, and analyze textual data.
[1] "(Feinerer, Hornik, and Meyer, 2008)"

#### The Data...
I've created a sample data file containing the first 100 lines of the US news corpus from the [HC Corpora](http://www.corpora.heliohost.org/) for the express purpose of exploring the `tm` package's capabilities.  The whole corpus is quite large and would pose a challenge to quickly obtaining some example results.  The data I use below can be found on [Github](https://github.com/suttonbm/suttonbm.github.io/tree/master/_source/datasciencecoursera-capstone/en-US.news.sample.txt).

#### Reading the Data
Let's get started.  First thing to do is install packages:


```r
install.packages("tm")
```

Next, we need to read in the data to be analyzed.  The `tm` package provides many different interfaces for reading in data.  In this case, we're just reading in a plain text file.


```r
US.news <- file.path("data")
US.news
```

```
## [1] "data"
```

```r
library(tm)
newsCorp <- Corpus(DirSource(US.news))
```

We can do some quick examination of the created corpus:


```r
as.character(newsCorp[[1]])[1:3]
```

```
## [1] "He wasn't home alone, apparently."                                                                                                                                                
## [2] "The St. Louis plant had to close. It would die of old age. Workers had been making cars there since the onset of mass automotive production in the 1920s."                        
## [3] "WSU's plans quickly became a hot topic on local online sites. Though most people applauded plans for the new biomedical center, many deplored the potential loss of the building."
```

The data has been read into a `Corpus` object.  The command above shows only the first three lines of the file.

#### Preprocessing Data
So now that our data is loaded into R, let's try some preprocessing.  The raw data may include extra whitespace or punctuation which could complicate or confuse the analysis.  Additionally, as discussed above, common words can be eliminated to simplify the dataset.

##### Removal of Punctuation

```r
newsCorp <- tm_map(newsCorp, removePunctuation)
as.character(newsCorp[[1]])[1:3]
```

```
## [1] "He wasnt home alone apparently"                                                                                                                                               
## [2] "The St Louis plant had to close It would die of old age Workers had been making cars there since the onset of mass automotive production in the 1920s"                        
## [3] "WSUs plans quickly became a hot topic on local online sites Though most people applauded plans for the new biomedical center many deplored the potential loss of the building"
```

##### Removal of Stopwords and Numbers

```r
newsCorp <- tm_map(newsCorp, removeWords, stopwords('english'))
newsCorp <- tm_map(newsCorp, removeNumbers)
as.character(newsCorp[[1]])[1:3]
```

```
## [1] "He wasnt home alone apparently"                                                                                                                          
## [2] "The St Louis plant   close It  die  old age Workers   making cars  since  onset  mass automotive production   s"                                         
## [3] "WSUs plans quickly became  hot topic  local online sites Though  people applauded plans   new biomedical center many deplored  potential loss   building"
```

##### Apply Stemming, Convert to Lowercase

```r
newsCorp <- tm_map(newsCorp, stemDocument)
newsCorp <- tm_map(newsCorp, tolower)
as.character(newsCorp[[1]])[1:3]
```

```
## [1] "he wasnt home alon appar"                                                                                                           
## [2] "the st loui plant   close it  die  old age worker   make car  sinc  onset  mass automot product   s"                                
## [3] "wsus plan quick becam  hot topic  local onlin site though  peopl applaud plan   new biomed center mani deplor  potenti loss   build"
```

##### Removing Extra Whitespace

```r
newsCorp <- tm_map(newsCorp, stripWhitespace)
as.character(newsCorp[[1]])[1:3]
```

```
## [1] "he wasnt home alon appar"                                                                                                   
## [2] "the st loui plant close it die old age worker make car sinc onset mass automot product s"                                   
## [3] "wsus plan quick becam hot topic local onlin site though peopl applaud plan new biomed center mani deplor potenti loss build"
```

### References
<p><a id='bib-JSSv025i05'></a><a href="#cite-JSSv025i05">[1]</a><cite>
I. Feinerer, K. Hornik and D. Meyer.
&ldquo;Text Mining Infrastructure in R&rdquo;.
In: <em>Journal of Statistical Software</em> 25.1 (2008), pp. 1&ndash;54.
ISSN: 1548-7660.
DOI: <a href="http://dx.doi.org/10.18637/jss.v025.i05">10.18637/jss.v025.i05</a>.
URL: <a href="https://www.jstatsoft.org/index.php/jss/article/view/v025i05">https://www.jstatsoft.org/index.php/jss/article/view/v025i05</a>.</cite></p>
